{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-taming-transformers.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_r4j0gvQgJ"
      },
      "source": [
        "# Colab-taming-transformers\r\n",
        "\r\n",
        "Original repo is located in [CompVis/taming-transformers](https://github.com/CompVis/taming-transformers) and [original colab can be found here](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb)\r\n",
        "\r\n",
        "My fork: [styler00dollar/Colab-taming-transformers](https://github.com/styler00dollar/Colab-taming-transformers)\r\n",
        "\r\n",
        "This is just a more compressed/smaller version of the original notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79RpJwq1s_tB"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwj8j_l201aF",
        "cellView": "form"
      },
      "source": [
        "#@title install\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "%cd taming-transformers\n",
        "!mkdir -p logs/2020-11-09T13-31-51_sflckr/checkpoints\n",
        "!wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt'\n",
        "!mkdir logs/2020-11-09T13-31-51_sflckr/configs\n",
        "!wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml'\n",
        "\n",
        "%pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "config_path = \"logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml\"\n",
        "config = OmegaConf.load(config_path)\n",
        "import yaml\n",
        "print(yaml.dump(OmegaConf.to_container(config)))\n",
        "\n",
        "from taming.models.cond_transformer import Net2NetTransformer\n",
        "model = Net2NetTransformer(**config.model.params)\n",
        "\n",
        "import torch\n",
        "ckpt_path = \"logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt\"\n",
        "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
        "missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "\n",
        "model.cuda().eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LiAOU6C-vTP",
        "cellView": "form"
      },
      "source": [
        "#@title load data\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "segmentation_path = \"data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png\" #@param {type:\"string\"}\n",
        "segmentation = Image.open(segmentation_path)\n",
        "segmentation = np.array(segmentation)\n",
        "segmentation = np.eye(182)[segmentation]\n",
        "segmentation = torch.tensor(segmentation.transpose(2,0,1)[None]).to(dtype=torch.float32, device=model.device)\n",
        "\n",
        "def show_segmentation(s):\n",
        "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0,:,:,None,:]\n",
        "  colorize = np.random.RandomState(1).randn(1,1,s.shape[-1],3)\n",
        "  colorize = colorize / colorize.sum(axis=2, keepdims=True)\n",
        "  s = s@colorize\n",
        "  s = s[...,0,:]\n",
        "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
        "  s = Image.fromarray(s)\n",
        "  display(s)\n",
        "\n",
        "show_segmentation(segmentation)\n",
        "\n",
        "c_code, c_indices = model.encode_to_c(segmentation)\n",
        "print(\"c_code\", c_code.shape, c_code.dtype)\n",
        "print(\"c_indices\", c_indices.shape, c_indices.dtype)\n",
        "assert c_code.shape[2]*c_code.shape[3] == c_indices.shape[1]\n",
        "segmentation_rec = model.cond_stage_model.decode(c_code)\n",
        "show_segmentation(torch.softmax(segmentation_rec, dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTfao3jJSCfW",
        "cellView": "form"
      },
      "source": [
        "#@title display random\n",
        "def show_image(s):\n",
        "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0]\n",
        "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
        "  s = Image.fromarray(s)\n",
        "  display(s)\n",
        "\n",
        "codebook_size = config.model.params.first_stage_config.params.embed_dim\n",
        "z_indices_shape = c_indices.shape\n",
        "z_code_shape = c_code.shape\n",
        "z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)\n",
        "x_sample = model.decode_to_img(z_indices, z_code_shape)\n",
        "show_image(x_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rVRrUOwbEH0",
        "cellView": "form"
      },
      "source": [
        "#@title generate image\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "idx = z_indices\n",
        "idx = idx.reshape(z_code_shape[0],z_code_shape[2],z_code_shape[3])\n",
        "\n",
        "cidx = c_indices\n",
        "cidx = cidx.reshape(c_code.shape[0],c_code.shape[2],c_code.shape[3])\n",
        "\n",
        "temperature = 1.0\n",
        "top_k = 100\n",
        "update_every = 50\n",
        "\n",
        "start_t = time.time()\n",
        "for i in range(0, z_code_shape[2]-0):\n",
        "  if i <= 8:\n",
        "    local_i = i\n",
        "  elif z_code_shape[2]-i < 8:\n",
        "    local_i = 16-(z_code_shape[2]-i)\n",
        "  else:\n",
        "    local_i = 8\n",
        "  for j in range(0,z_code_shape[3]-0):\n",
        "    if j <= 8:\n",
        "      local_j = j\n",
        "    elif z_code_shape[3]-j < 8:\n",
        "      local_j = 16-(z_code_shape[3]-j)\n",
        "    else:\n",
        "      local_j = 8\n",
        "\n",
        "    i_start = i-local_i\n",
        "    i_end = i_start+16\n",
        "    j_start = j-local_j\n",
        "    j_end = j_start+16\n",
        "    \n",
        "    patch = idx[:,i_start:i_end,j_start:j_end]\n",
        "    patch = patch.reshape(patch.shape[0],-1)\n",
        "    cpatch = cidx[:, i_start:i_end, j_start:j_end]\n",
        "    cpatch = cpatch.reshape(cpatch.shape[0], -1)\n",
        "    patch = torch.cat((cpatch, patch), dim=1)\n",
        "    logits,_ = model.transformer(patch[:,:-1])\n",
        "    logits = logits[:, -256:, :]\n",
        "    logits = logits.reshape(z_code_shape[0],16,16,-1)\n",
        "    logits = logits[:,local_i,local_j,:]\n",
        "\n",
        "    logits = logits/temperature\n",
        "\n",
        "    if top_k is not None:\n",
        "      logits = model.top_k_logits(logits, top_k)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    idx[:,i,j] = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    step = i*z_code_shape[3]+j\n",
        "    if step%update_every==0 or step==z_code_shape[2]*z_code_shape[3]-1:\n",
        "      x_sample = model.decode_to_img(idx, z_code_shape)\n",
        "      clear_output()\n",
        "      print(f\"Time: {time.time() - start_t} seconds\")\n",
        "      print(f\"Step: ({i},{j}) | Local: ({local_i},{local_j}) | Crop: ({i_start}:{i_end},{j_start}:{j_end})\")\n",
        "      show_image(x_sample)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}